var suggestions=document.getElementById('suggestions'),search=document.getElementById('search');search!==null&&document.addEventListener('keydown',inputFocus);function inputFocus(a){a.ctrlKey&&a.key==='/'&&(a.preventDefault(),search.focus()),a.key==='Escape'&&(search.blur(),suggestions.classList.add('d-none'))}document.addEventListener('click',function(a){var b=suggestions.contains(a.target);b||suggestions.classList.add('d-none')}),document.addEventListener('keydown',suggestionFocus);function suggestionFocus(c){const d=suggestions.classList.contains('d-none');if(d)return;const a=[...suggestions.querySelectorAll('a')];if(a.length===0)return;const b=a.indexOf(document.activeElement);if(c.key==="ArrowUp"){c.preventDefault();const d=b>0?b-1:0;a[d].focus()}else if(c.key==="ArrowDown"){c.preventDefault();const d=b+1<a.length?b+1:b;a[d].focus()}}(function(){var a=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:'id',store:["href","title","description"],index:["title","description","content"]}});a.add({id:0,href:"/risk-measurement/docs/estimation/expert-elicitation/",title:"Expert Elicitation",description:"We have to make decisions about risks\u0026hellip; often with imperfect information. Data may not be available, may not be timely to gather, or measurement options may not yet exist. We may be left to use data with only a loose relationship with the target problem, or nothing at all.\nThe response to these situations may be to trust experts who have developed the most intuition around these risks and elicit numerical values from them.",content:"We have to make decisions about risks\u0026hellip; often with imperfect information. Data may not be available, may not be timely to gather, or measurement options may not yet exist. We may be left to use data with only a loose relationship with the target problem, or nothing at all.\nThe response to these situations may be to trust experts who have developed the most intuition around these risks and elicit numerical values from them. They may adjust reference class data, or provide a best estimate through a structured approach.\nThe elicitation of numerical values from experts has many known approaches. These values can be used for all sorts of risk measurement, and the approaches increase in structure depending on the need for rigor.\nRigor # Watercooler estimations are encouraged. Casual discussions can help build culture around estimation skill. Structured elicitation can increasingly include the following techniques to support high impact decision making or dangerous risks.\n First, the reduction of bias is common in elicitation methods. Elicitation is a human, subjective process that can be guarded against harmful influence. A panel of experts can have values combined to better represent a consensus view and eliminate bias from a single individual. Questions and feedback is structured to promote learning before concluding. The debate between experts is often managed closely to protect participants from unhealthy influence. Experts are trained for probabilistic calibration and forecasting skill. Data is carefully introduced as a reference class, or monte carlo simulations are developed to support judgement. Forecasting platforms like the Good Judgement Open are examples of elicitation at scale.  As you may imagine, Elicitation that incorporates all of these factors would be time consuming, resource heavy, and expensive to organize.\nSimple Elicitation # A simple, workplace approach to expert elicitation looks as follows:\n Clearly define the desired values or distribution. Train expert(s) for elicitation. Elicit a preliminary value from experts. If time allows, encourage discussion between experts (if any) If time allows, elicit a final value.  Other methods # These are formal methods that look to preserve scientific integrity or assist with defensible policy making. They are procedurally \u0026ldquo;heavy\u0026rdquo;.\n Delphi (paper) is used by RAND and has a colorful history in many industries. Is known for heavily protecting participants and structuring Q\u0026amp;A between elicitations. Cooke\u0026rsquo;s \u0026ldquo;classical\u0026rdquo; method is well known in environmental sciences and is used for applying stronger weighting to experts who perform better in calibration exercises. The SHELF method has visible usage and journal references but is not yet understood by this documentation\u0026rsquo;s authors.  "}).add({id:1,href:"/risk-measurement/docs/intro/introduction/",title:"Introduction",description:"Risk Measurement teaches security engineers about quantitative risk.\nExamples: Find your a-ha! moment by looking through creative approaches to problems.\nGuidance: Avoid gotchas and pitfalls that make risk measurement treacherous.\nManagement: Find the optimum place for risk measurement with groups of people in a workplace.\nReading material is suggested throughout this documentation for those who want to dive deeper into the concepts, or see Reading Material.\nSee the blog for examples and discussion with current events.",content:"Risk Measurement teaches security engineers about quantitative risk.\nExamples: Find your a-ha! moment by looking through creative approaches to problems.\nGuidance: Avoid gotchas and pitfalls that make risk measurement treacherous.\nManagement: Find the optimum place for risk measurement with groups of people in a workplace.\nReading material is suggested throughout this documentation for those who want to dive deeper into the concepts, or see Reading Material.\nSee the blog for examples and discussion with current events.\n"}).add({id:2,href:"/risk-measurement/docs/examples/vulnerability/",title:"Vulnerability",description:"Measuring a vulnerability's risk",content:"You\u0026rsquo;ve found a bug. We\u0026rsquo;ll provide an example bug. Forget any previous vulnerability risk framework you\u0026rsquo;ve learned over the years, including color coding, zero-to-ten scales, or high/medium/low. We\u0026rsquo;re going to think about the bug critically and communicate the nuances that distinguish this bug from other bugs.\n Example: We\u0026rsquo;ve found an internet exposed Jenkins server (build001) with the scripting console accessible.\n Probably familiar to you. A very common vulnerability many of us have run into. Our build environment is a Jenkins is a CI/CD server that companies use to continuously build software. The scripting console is basically a web based shell available to an attacker, giving remote code execution.\nSounds like a critical finding. Right? Under normal circumstances this is a \u0026ldquo;just fix it\u0026rdquo; vulnerability that shouldn\u0026rsquo;t require a whole lot of debate to prioritize. Or\u0026hellip; so you\u0026rsquo;d think.\nWe register the vulnerability in build001 as VULN-0 in some fancy task management sytem or tracker.\nHistorically, you might write label the bug as critical, Unbreak Now!, CVSS 10, P0, or other terms I\u0026rsquo;d like you to forget for now. Despite these scary sounding labels:\n Another team might need to be tapped to investigate and fix it. This other team might not share your sense of urgency. There may be other pressing tasks (or vulnerabilities) being mitigated. Your team (and others) may be swimming in critical bugs. Other teams disagree about criticality.  Now there\u0026rsquo;s a need to step into this vulnerability and communicate it more specifically. With analysis, we will tease out and communicate the intricacies of this vulnerability that are causing us, as experts, to find this vulnerability undesireable.\nFirst, this bug is internet facing. Literally anyone can find this vulnerabilty. Plus, there\u0026rsquo;s plenty of tooling available to find it efficiently. So, we want to communicate the simplicity of which this can be found. Exploitation is also simple - it\u0026rsquo;s literally designed to be used as scriptable access to the underlying system.\nLet\u0026rsquo;s think of this in terms of time to exploitation and we can model a scenario as needed to communicate it. What are the odds this will be exploited within 30 days? The timeframe is arbitrary and we can pick any term. We\u0026rsquo;re looking to communicate a short term urgency. So, maybe we choose something like 30 days.\nNow we have a scenario and can forecast against it properly as we have learned. Let\u0026rsquo;s say the forecast comes in at 45%.\n Scenario: A SEV0 involving VULN-0 exploitation occuring within 30 days.\nForecast: 45% Yes\n  Second, the server is a build system that might be a dumpsterfire of ACL exceptions, credentials, intellectual property (source code). The compromise of the host itself is not a disclosable incident. Notably, user data is just a stone\u0026rsquo;s throw away. Lateral movement towards protected data would be trivial for an attacker.\nWe will create a new forecast, preceded with a condition that an adversary has already exploited this vulnerability on build001. There\u0026rsquo;s nuance to possible outcomes:\nAn adversary might not be entirely concerned about your data - especially one that discovers this vulnerability from broad scanning. Maybe they\u0026rsquo;ll install a cryptominer and move on\u0026hellip; or not. So, this is probabilistic as well. Since we\u0026rsquo;re working with examples, we\u0026rsquo;ll suggest the forecast comes in at 60%.\n Condition: VULN-0 was exploited and discovered.\nScenario: Incident response will require disclosure.\nForecast: 60%\n  Lastly, and most concerning\u0026hellip; it\u0026rsquo;s probably compromised already! It\u0026rsquo;s vulnerable now, not in the future. Let\u0026rsquo;s pretend that this vulnerability has existed for a year or more. Yikes! We will still create a scenario for a current state, so long as it\u0026rsquo;s not yet confirmed.\n Scenario: An investigation will reveal that build001 is compromised.\nForecast: 80%.\n  Now we have three forecasts, summarized here for readability.\n Vulnerability: VULN-0\nDescription: Our build host build001 has a globally accessible shell scripting console.\nScenario: We believe VULN-0 will be exploited within 30 days.\nForecast: 45%\nCondition: VULN-0 was exploited. Scenario: Incident response will require disclosure.\nForecast: 60%\nScenario: An investigation will find build001 compromised.\nForecast: 80%\n  Exploitation has a 45% chance with a 60% the resulting incident would be disclosable in 30 days. This works out to a 45% x 60% = 27% chance that a disclosable incident will occur within 30 days (if it hasn\u0026rsquo;t already).\nHopefully the parties who review these measurements will be convinced about the nature of the risk involved and will help with a fix. Now, we prepare for the event that they are not fixed.\n"}).add({id:3,href:"/risk-measurement/docs/examples/vulnerability-impact/",title:"Vulnerability Impact",description:"Measuring vulnerability impact.",content:"Impact can be frustrating to work with because a variety of undesirable outcomes may or may not happen. Unless we\u0026rsquo;re forced to translate everything into dollar values, we can work with whatever quantities we\u0026rsquo;re concerned about.\nExample Scenarios # Let\u0026rsquo;s assume the following scenario:\n Scenario: An SQL injection is discovered in our product.  These scenarios assume that an SQL injection has already been exploited.\n Users Compromised: Sensitive data for more than 100 customers will be impacted. Downtime: Downtime for incident response will exceed 1 hour. Engineering Response Hours: More than 8 engineering hours will be spent on response. Growth Accounting: A reduction from projected new, expanded, or retained by more than 1%. Headlines: One or more national newspaper headlines.  Example Intervals # Another step up is to elicit a credible interval. For instance, a min-and-max range (5/95% percentile) where we believe the true value will lane.\n Scenario: A session secret leaked has leaked to an adversary.  Here\u0026rsquo;s an example impact assessment with intervals that assume the secret has already leaked.\n75% Interval forecasts #  Users Compromised: 0-1.5M Customers (75% Interval) Downtime: 0-9 days (75% Interval) Engineering Response Hours: 40-500 hours (75% Interval) Growth Accounting: 1k-100k Churn this month (75% Interval) Headlines: 0-5 Headlines (75% Interval)  It\u0026rsquo;s possible that these can all be decomposed further into a uniform dollar amount. However, the abstractions themselves are likely to be informative enough for decision making and the decomposition work will require time and effort.\n"}).add({id:4,href:"/risk-measurement/docs/examples/mitigation/",title:"Mitigation",description:"Measuring a mitigation's impact.",content:"Let\u0026rsquo;s assume we found a vulnerability on a host named build001.\n👉  This example vulnerability was discussed here.  The vuln # The vuln is bad. It\u0026rsquo;s an RCE exposed to the internet, allowing for remote compromise right now. It\u0026rsquo;s a feature of our CI/CD system, and is effectively a remotely accessible shell running as root.\nThere are complicating factors:\n Several partners companies remotely access build001 The script console that offers a shell to the host is used regularly. No engineer currently owns this system. The whole system will be replaced with a new platform build002 in 6mo, suggesting our efforts need to make sense in a short term.  This makes the job tough! Well, we can\u0026rsquo;t efficiently model each of these limitations in a reasonable time. We\u0026rsquo;re here just to focus on risk, anyway.\nThis vulnerability was previously measured as follows:\n There\u0026rsquo;s a 45% chance that VULN-0 will be exploited within 30 days. (If Exploited) Incident response has a 60% chance of requiring disclosure. An investigation has an 80% chance of finding build001 compromised.  Any possible mitigation has to have a positive effect on these measurments\u0026hellip; but by how much?\nThe target # Our role as the trusted and informed product security expert is to set some goalposts on what our mitigation should get us to. Let\u0026rsquo;s do that now.\nPrevious work has already measured the vulnerability as-is with at having a 60% chance of being involved in a disclosable incident. (see the Vulnerability example)\nAfter a bunch of brainstorming, let\u0026rsquo;s say we come up with the following goal:\n📉  Reduce the probability of a disclosable incident to 5% or lower within the next 30 days.  This number was arrived at based on the opinion of yourself, or your group, that some method exists to hit this target. Now we just have to make those plans.\nThe plans # Our quant efforts don\u0026rsquo;t spit out plans, we do. We\u0026rsquo;ve set these goalposts for ourselves, but we still require an achievable plan to get there. Let\u0026rsquo;s assume the following plan is suggested by the team:\n Bring build001 down for maintenance and investigation. Create a network ACL limited to partner access. Reduce authorization in credentials on build001, bring back online. Prioritize the migration off of build001 and onto build002  This fix doesn\u0026rsquo;t totally eliminate the vulnerability! As written, it\u0026rsquo;s better described as a workaround. Should we push for this to be accepted as the plan?\nLet\u0026rsquo;s re-forecast our previous measurements, with a condition that our mitigations are put in place. Here\u0026rsquo;s an example:\n There\u0026rsquo;s a 45% 5% chance that VULN-0 will be exploited within 30 days. (If Exploited) Incident response has a 60% 30% chance of requiring disclosure. An investigation has an 80% (No Change) chance of finding build001 compromised.  Let\u0026rsquo;s talk through these changes.\nRegardless of how they came upon these numbers: The team is expressing confidence that exploitation is less likely with network ACL\u0026rsquo;s (going from 45% to 5%)\nNext, A disclosure event seems less likely with the reduced credential footprint, (but not eliminated!), so the reduced forecast 60% to 30% is reasonable to see a reduction there.\nLast, there is no change in whether it is currently compromised or not. This makes sense, since new mitigations can\u0026rsquo;t change whether it is compromised right now or not.\nThe decision # Now that the reasoning for these reductions are understood, we can make a decision on whether these risks are under our threshold. There\u0026rsquo;s now a 5% of exploitation in 30 days. If this happens, there\u0026rsquo;s a 30% chance it would result in a disclosable event. When worked out, this gives us 5% x 30% = 1.5%, or a 1.5% chance of a disclosable event which is well below the threshold we set for an acceptable decision.\nAssuming the group agrees to the approach and the work, we have back-of-napkin\u0026rsquo;d our model of the risks that matter to us and met a target we are good with.\n"}).add({id:5,href:"/risk-measurement/docs/estimation/calibration/",title:"Scoring and Calibration",description:"Scoring and calibration mitigates the largest weaknesses of qualitative risk methods and empowers us to correct ourselves over time. Investing in ongoing correction is an opportunity to shine over other industries that have difficulty doing so, for whatever reasons.\n Remarkably, most intelligence organizations do not proactively track their forecasting accuracy and, therefore, do not know how accurate their forecasts are or what types of biases intelligence analysts (or organizations) might exhibit.",content:"Scoring and calibration mitigates the largest weaknesses of qualitative risk methods and empowers us to correct ourselves over time. Investing in ongoing correction is an opportunity to shine over other industries that have difficulty doing so, for whatever reasons.\n Remarkably, most intelligence organizations do not proactively track their forecasting accuracy and, therefore, do not know how accurate their forecasts are or what types of biases intelligence analysts (or organizations) might exhibit. (link)\n Error # Probabilistic statements are uncertain and produce error. All probabilistic statements that are not certain (A full 100% or 0%) result in error. The risks we measure will hopefully have observable outcomes, and we can apply truth to these statements and calculate error, or a score.\nScore (or \u0026ldquo;error\u0026rdquo;): The accuracy of an individual forecast when compared to hindsight.\nChicken Little 🐤 is an example of a low scoring forecast. Chicken Little says that the sky is absolutely going to fall, and then the sky does not fall. The forecast is incorrect and is scored poorly. Chiken Little is a demonstrably poor source of information.\nCalibration: How trustworthy the source of a forecast is.\nThe Boy Who Cried Wolf 🐺 is an example of poor calibration. Repeated, high confidence claims that turn out to be false will result in poor calibration and would attract correction. A poorly calibrated individual may frequently use the phrase \u0026quot;I'm 90% sure\u0026quot; and end up being correct a mere 10% of the time.\nDecades of research show that humans are poorly calibrated without training, practice, or corrective feedback loops. (See: Meehl, Tetlock and Kent).\nResearch shows that individuals are calibrated with minimal training, and regular practice supports this as well. (See: Tetlock. Good Judgement)\nKeeping Score # Forecasts that include their associated confidence can make use of the Brier Score to record accuracy over time. This is simply calculated as the \u0026quot;Squared Error\u0026quot;.\nThe Good Judgement Open has an accessible definition of the Brier Score: :\nThe Brier score is the squared error of a probabilistic forecast. To calculate it, we divide your forecast by 100 so that your probabilities range between 0 (0%) and 1 (100%). Then, we code reality as either 0 (if the event did not happen) or 1 (if the event did happen). For each answer option, we take the difference between your forecast and the correct answer, square the differences, and add them all together. For a yes/no question where you forecasted 70% and the event happened, your score would be (1 – 0.7)2 + (0 – 0.3)2 = 0.18. For a question with three possible outcomes (A, B, C) where you forecasted A = 60%, B = 10%, C = 30% and A occurred, your score would be (1 – 0.6)2 + (0 – 0.1)2 + (0 – 0.3)2 = 0.26. The best (lowest) possible Brier score is 0, and the worst (highest) possible Brier score is 2. An average Brier score is useful for tracking the reliability of a forecaster. It can be tracked by certain topics, panels, individuals, etc.\nFor instance, let's take a batch of some pretty good weather predictions.\n   Forecast % Rain % No Rain Outcome Brier Score Brier Score (Work)     1 0.99 0.01 Yes (1) 0.0002 (1-.99)^2+(0-.01)^2   2 0.8 0.2 Yes (1) 0.08 (1-.8)^2+(0-.2)^2   3 0.334 0.666 No (0) 0.223112 (0-.334)^2 + (1-.666)^2   4 0.01 0.99 No (0) 0.0002 (0-.01)^2 + (1-.99)^2   5 0.95 0.05 Yes (1) 0.005 (1-.95)^2 + (0-.05)^2    This table shows an average Brier Score of 0.0617024. If we observed this forecast score from our local meteorologist, we'd be pleased and consider this forecast source useful. Let's put together a table of pretty terrible weather forecasts for comparison.\n   Forecast % Rain % No Rain Outcome Brier Score Brier Score (Work)     1 0.1 0.9 Yes (1) 1.62 1(1-.01)\\^2+(0-.9)^2   2 0.04 0.96 Yes (1) 1.8432 (1-.04)^2+(0-.96)^2   3 0.77 0.23 No (0) 1.1858 (0-.77)^2+(1-.23)^2   4 0.88 0.12 No (0) 1.5488 (0-.88)^2+(1-.12)^2   5 0.2 0.8 Yes (1) 1.28 (1-.2)\\^2+(0-.8)\\^2    This table shows an average brier score of 1.49556. Any reasonable individual would consider those forecasts not useful.\nYour industry will vary on what a \u0026quot;useful\u0026quot; threshold for a forecast source would be. For instance, a Brier Score that forecasts data related to part failures and explosions will be very different from a risk forecast about missed project deadlines. This documentation leaves that up to the engineers involved to set their requirements.\nHowever, all industries can agree that engineers seeing a reduction of a Brier Score over time is a favorable trend.\nForecast sources can also be compared with the \u0026quot;Brier Skill Score\u0026quot;, in which we can discover better risk prediction models or methods. This is heavily used in meteorology to compare the value of a predictive model to a tried and true model, like a simple historical average. It is expressed simply with two Brier scores being compared below.\n BrierSkillScore = 1.0 – BrierScoreNew / BrierScoreReference\n Calibration # Calibration is best observed with a line chart. We graph a lot of forecasts with their average correctness, in buckets of certainty. We can then make statements like:\n \u0026ldquo;When we\u0026rsquo;re 90% sure of a future event, we\u0026rsquo;re correct 90% of the time.\u0026rdquo;\n This chart shows that the source of forecasts (a person, a statistical model) is perfectly calibrated. When they are 10% confident, they are historically 10% correct. When they are 0% or 100% certain of an event, it will absolutely shake out that way with no mistakes.\n  A perfectly calibrated source of forecasts could be seriously valuable. The source\u0026rsquo;s suggestions would come with great authority (especially with bold forecasts at 0% or 100%). This is not common in low information situations (like risk) and especially when dealing with human forecasts.\nThe following chart demonstrates an unreliable source of forecasts. The chart communicates to us that we can\u0026rsquo;t trust the judgement of the source of forecasts. The source may claim some percentage of certainty towards an outcome while their track record doesn\u0026rsquo;t support it.\n  A more realistic and well calibrated source may have some error, but not significant, like the following:\n  This sort of calibration is what you might find from human sources who have been trained and have been corrected over time by calibrated themselves.\nVolume required for calibration # Calibration can be difficult to analyze without a large volume of forecasts. Forecasts with 50% certainty can be assessed quickly, while forecasts with 99% certainty may require hundred(s) of forecasts to properly detect if they are truly have one-in-one-hundred errors or not.\nIf calibration is an important value in your risk assessment methodology, you can standardize a lower level of certainty (say, 70%) to increase the ability to audit calibration.\n"}).add({id:6,href:"/risk-measurement/docs/estimation/percentiles/",title:"Percentiles",description:"A percentile estimate is a probability and value pulled from a distribution. Some examples:\n The 90% percentile of customer losses is $1M.\n This communicates a 90% chance that future customer losses will be $1M or less.\nIt also communicates a 10% chance that losses will exceed $1 million.\nThe upside: A percentile estimate is very simple to gather and communicate.\nThe downside: It won\u0026rsquo;t communicate how the risk behaves.",content:"A percentile estimate is a probability and value pulled from a distribution. Some examples:\n The 90% percentile of customer losses is $1M.\n This communicates a 90% chance that future customer losses will be $1M or less.\nIt also communicates a 10% chance that losses will exceed $1 million.\nThe upside: A percentile estimate is very simple to gather and communicate.\nThe downside: It won\u0026rsquo;t communicate how the risk behaves.\nStated differently: The possible outcomes might be normally distributed, exponential, uniform, or something else. This sort of intuition is not communicated from from a single percentile estimate.\nSome more examples of what a percentile estimate can sound like:\n We expect 95% of fraud claims to lie under $1M. The median (50% percentile) customer account value is $1K. 75% of our incidents last under 3 days to investigate.  Elicitation is quick and often understood. Though, a sole percentile estimate won\u0026rsquo;t disclose other information about a risk (extreme values or underlying distributions).\nFor instance, let\u0026rsquo;s say the 90% height in a group of people happens to be 6 foot 3. Knowing that height is normally distributed, we don\u0026rsquo;t expect the remaining 10% of people to include any 20 foot giants. Rather, we expect the remaining 10% of people to be able to walk through a doorway.\nHowever, a 90% percentile estimate of regualtory settlements may be entirely different. Assume 90% of settlements are expected to be under $200 million for a business. Within that remaining 10% of possible settlements may be some extreme values in the multiple billions.\nThe percentile itself does not communicate distributions of risk. Still, a percentile estimate is quick and useful especially if an audience is familiar with the underlying behavior of the risk.\n"}).add({id:7,href:"/risk-measurement/docs/estimation/intervals/",title:"Intervals",description:"An interval estimate can be described as a range of values, plus the probability that an outcome will fall within them. More formally, these can be called probability intervals, or credible intervals.\nIntervals can also be thought of as the values between two percentile values on a distribution.\nHere are the exact same statements with increasing formality. Each communicates an interval:\nHere\u0026rsquo;s a statement that could be an interval with one more bit of information:",content:"An interval estimate can be described as a range of values, plus the probability that an outcome will fall within them. More formally, these can be called probability intervals, or credible intervals.\nIntervals can also be thought of as the values between two percentile values on a distribution.\nHere are the exact same statements with increasing formality. Each communicates an interval:\nHere\u0026rsquo;s a statement that could be an interval with one more bit of information:\n After a DDoS\u0026hellip; we could be down from about a minute to seven days\n If we get a percentage from the individual, we can elicit an interval estimate.\n There\u0026rsquo;s a 90% chance a DDoS causes downtime between 1 minute and 7 days.\n Cool. An interval is just a pair of percentile estimates. Knowing what is going on underneath, we can word these in a variety of different ways\u0026hellip;\n 5% of DDoS downtime will be below 1 minute. 5% will be above 7 days.\n An interval should note what percentiles it represents. (For instance, 5% and 95%). It is generally assumed in this documentation that the interval is symmetric and represents the middle of the distribution you\u0026rsquo;re discussing.\nMore on Intervals # A credible interval represents a range of possible values, and also includes a percentage belief (confidence) that the outcome will fall into it.\nIncreasing your efforts to study a risk may change an interval estimate in a couple ways - by changing the interval or by changing the belief (probability) associated with it.\nThe size of the interval may change: This means the possible outcomes become wider.\nFor example, there may be a 90% chance that 10-20 employee laptops are unencrypted. After some research, this may shrink to a 90% chance that 1-10 employee laptops are unencrypted.\nThe probability of the interval: This means that the odds are different for the same possible outcomes.\nFor instance, a 90% chance of settlement costs between $10M-$100M dollars might reduce to 80% after some effort\u0026hellip; but the interval you indicated ($10M-$100M) stays the same.\nHere\u0026rsquo;s a longer form example. An investigation has discovered an insider threat who was caught remotely accessing a co-worker\u0026rsquo;s laptop. There\u0026rsquo;s evidence to suggest they may have accessed more systems, but it\u0026rsquo;s not definitive.\nHere is a scenario they propose:\n Scenario : Number of employee systems accessed by the insider within the last six months.  An interval estimate could read as A 95% chance of 5-10 employee owned systems. Given this measurement from a security team, an incident command might allocate forensic resources to determine what may have been done.\nA visual example of a percentage belief that an unknown value will end up within this range when revealed:\n95% Certainty │ │ │ │ │ ▼ 5 10 ▽──────────────▽ ◀─────────────────────────────────────────────────────────────────────▶ ... -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ... In summary, an interval estimate provides:\n An interval (a lower and upper value representing a range of values) A percentage belief that an outcome lies within that interval.  "}).add({id:8,href:"/risk-measurement/docs/examples/breach-impact/",title:"Breach Impact",description:"Measuring the impact from a breach.",content:"We may be asked what the impact would be if we suffered a breach. The following questions are useful to ask when thinking through a model.\n Did we discover this breach? Are we talking about response costs? Are we talking about costs from an undiscovered breach?  IP theft / Corporate espionage / competitive threats   Do we strictly mean financial costs?  Or measurements in engineering hours, days offline, etc   Are we counting costs imposed to others (imposed risk)? Are we also speculating on legal costs? Are we talking about operational costs? Do sales and growth from reptuation hits and future business losses count?  Example forecasts #  There\u0026rsquo;s a 30% chance we\u0026rsquo;d have to pay a fine to a regulator If the breach accessed a host in production, 85% chance we\u0026rsquo;d disclose to a regulator. If the breach accessed a dev host, a 15% chance we\u0026rsquo;d disclose to a regulator  Example intervals #  A fine to a regulator would be between $100K and $300M (95%) The total costs of our breach would land between $500K-$80M (90%) We\u0026rsquo;d spend between 5 and 200 engineer days on incident response. (95%) There could be zero newspaper headlines, or up to 10 (70%) Our customer churn would be between 5-12% the month after public disclosure. (75%)  Example distributions #  A legal settlement would follow a lognormal distribution with an average of $1M and a 5% chance of exceeding $100M A legal settlement would follow a pareto distribution with an average of $1M and a 5% chance of exceeding $100M A legal settlement would follow a zipf\u0026rsquo;s distribution with an average of $1M and a 5% chance of exceeding $100M.  "}).add({id:9,href:"/risk-measurement/docs/examples/new-product-review/",title:"New Product Review",description:"Measuring risk of future products.",content:"Let\u0026rsquo;s imagine a new product. We\u0026rsquo;ll discuss some hypothetical risk scenarios, and then discuss some estimations for them.\n Scenario: We launch a feature that lets customers upload profile pictures.  Discussion # This change allows \u0026ldquo;User Generated Content\u0026rdquo; or UGC onto a platorm. UGC can have multi-dimensional impacts.\nAllowing new UGC into a product workflow creates many operational risks that will result during the lifetime of the product, as well as risks that would be infrequent but potentially devastating. For instance, a vulnerability in image processing dependencies.\nIf we haven\u0026rsquo;t launched the product yet, we can only speculate as to what may happen when we do. Rather than being the security expert who says photo uploads are hella bad!, we discuss the risks point by point and brainstorm mitigation options. Here are various studies we could engage in and elicit estimates for:\nInterval Examples #  How many reports to NCMEC in the first 60 days? (Min-Max 75%) How many active CP/CSAM Investigations in the first 60 days? (Min-Max 75%) What increase in support tickets in the first 60 days? (Min-Max 75%) How many Image Codec SEVs in the first 60 days? (Min-Max 75%) How many Image Permission SEVs in the first 60 days? (Min-Max 75%)  Scenario Examples #  Will an Image Code RCE SEV result in a disclosable incident in 365 days? Will negative newspaper headlines appear about this feature in the first 60 days? Will an adversary deletion SEV require us to restore from images from backups in the first 60 days? Will we have to contact law enforcement as a result of this product in the first 60 days?  Distribution Examples #  How many reports to NCMEC in the first 60 days? (Pareto) How much storage in S3 will be occupied by this product? (Lognormal)  "}).add({id:10,href:"/risk-measurement/docs/examples/registers/",title:"Registers",description:"Reviewing multiple risks in a single place.",content:"Many risks can be compared quantitatively so long as they share a measurement.\nLet\u0026rsquo;s compare a bunch of vulnerabilities we are treating as risks. We assume we have VULN-0 to VULN-9. If we have a common group of forecasts for all of them, we can sort and manage them as you\u0026rsquo;d expect.\n   Vulnerability (1 year) Exploited? (Exploited) Dislosable? Headline?     VULN-0 2.00% 5.00% 5.00%   VULN-1 3.00% 1.00% 1.00%   VULN-2 1.00% 1.00% 1.00%   VULN-3 2.00% 70.00% 1.00%   VULN-4 30.00% 10.00% 40.00%   VULN-5 10.00% 1.00% 5.00%   VULN-6 6.00% 14.00% 10.00%   VULN-7 1.00% 100.00% 5.00%   VULN-8 15.00% 15.00% 1.00%   VULN-9 2.00% 25.00% 1.00%    Register approaches are extremely cumbersome if direct elicitation is required for every entry.\nThe layout may convince you that a fixing-from-the-top-down approach is the obvious one, and that we should discover all the risks, elicit forecasts for all the risks and register all the risks. Of course, this is not the right way to go.\nActual decisions involving these risks will always include non-registered factors. Engineers might not have a clear mitigation in mind. There may be costs we haven\u0026rsquo;t enumerated. Maybe delays are involved with some of the risks we\u0026rsquo;d prefer mitigating.\nA risk register will not lay plans in front of us, so care should be applied in efforts to tie everything into a register.\nHowever, there are ways to make registers more efficient.\nLens Models # A lens model is a technique that could help populate a risk register more effiently. It allows the creation of a statistical model that captures expert opinion (or plural from a panel) on a few select predictors and outputs a register of probabilities to prioritize from. An example of this is here: AWS Risk Model\nStatistical models # Third party vulnerabilities are well covered with a method like EPSS which is promising for register based approaches. These are useful when a corpus of data is available and closely relevant to the target scenario you are forecasting. These approaches lose value in first party vulnerability risk areas or more obscure software without openly available vulnerability data.\n"}).add({id:11,href:"/risk-measurement/docs/examples/team-direction/",title:"Team Direction",description:"Measuring risk of future products.",content:"We want our teams to work in a unified direction. This section describes codifying that direction as a risk. Codifying a risk is similar to selecting a KPI or setting an objective.\nThese directional risks should be broad. There shouldn\u0026rsquo;t be many of them, as we\u0026rsquo;re using it to provide a meaningful north star. Using incident classification (SEV, P0, etc) helps clarify what incident means, as well as thresholds of other undesirable things like costs or losses.\nSome ideas on directional scenarios:\n \u0026gt; N regrettable customer exits resulting from a SEV0. Any party in {set of regulators} formally discusses a SEV0 with us. A SEV0 with \u0026gt;$X Million in losses. Any {set of bloggers and newspapers} publishes commentary on a SEV0. A SEV0 has confirmed, unauthorized access to customer data. \u0026gt;% of total users impacted by a SEV0 involving an explicitly defined failure.  These are examples and some will resonate while others will not. The creation and selection of a good scenario is a leadership task.\nFrom here, decomposition allows us to sniff out hot areas of risk where large causal areas form. We can do this so long as we follow probabilistic axioms, especially the third axiom of mutually exclusive events. This means our decomposition needs to be Mutually Exclusive and Collectively Exhaustive (MECE in business speak).\ngraph TD; A[SEV0 \u0026gt; $10M]--\u0026gt;B[Other]; A--\u0026gt;C[Application Failure]; A--\u0026gt;D[Infrastructure Failure]; A--\u0026gt;F[IT Failure] A--\u0026gt;E[Insider]; Let\u0026rsquo;s assume we have forecasts to support this breakdown. Because we\u0026rsquo;re breaking down all possibilities for the SEV0 \u0026gt; $10M scenario, the following need to equal 100%.\n 🏆 IT Failure: 40% Application Failure: 25% Infrastructure Failure: 20% Other: 10% Insider: 5%  It looks like IT Failures make up a lot of the risk here. But why do all of this work?\nManagement-through-metrics is a focus area where troubling patterns can sometimes be found in an organization. Be careful with performance incentives based on reductions to subjective risk measurements.\nThere are many issues with strict quantitative leadership which may make it into this documentation in the future.\n Risk based knowledge work resists simple performance measurement. Knowledge workers are trusted to measure and manage themselves. OKRs and peer reviews are crucial for evaluating a knowledge worker. Objective measurement is efficient, but risk is a subjective concern. Overly quantitative management becomes subjected to Goodhart’s Law.  Rather, we are concerned about encouraging and organizing behaviors that decompose the KPI. A quantitative risk used as a KPI provides direction, and creates an organizational obligation to discover where the contributors to that risk come from.\nFor instance, a high performing engineer will demonstrate the relationship of their efforts to how their efforts in finding and mitigating risks relate to informing the KPIs.\nSimilarly, a directional risk scenario can help engineers self-organize around clearly identified risks and the causes that contribute to them. Product organizations do this similarly with directly measurable metrics (Monthly Recurring Revenue, Monthly Active Users, etc) and we\u0026rsquo;re simply doing the same with a synthetic metric.\n"}).add({id:12,href:"/risk-measurement/docs/intro/industry/",title:"Industry",description:"Cyber security is compared to other industries for its lack of rigorous and quantitative analysis of risk. Risk Measurement focuses on quantitative risk measurement to assist with this imbalance. While cyber security may lacks the forms of rigor found in other industries, this documentation also seeks to mitigate known weaknesses found in those approaches.\nWhat makes other industries quantitative about how they handle risk? How are we so different? Well, here are some examples, and we\u0026rsquo;ll start with one you see all the time.",content:"Cyber security is compared to other industries for its lack of rigorous and quantitative analysis of risk. Risk Measurement focuses on quantitative risk measurement to assist with this imbalance. While cyber security may lacks the forms of rigor found in other industries, this documentation also seeks to mitigate known weaknesses found in those approaches.\nWhat makes other industries quantitative about how they handle risk? How are we so different? Well, here are some examples, and we\u0026rsquo;ll start with one you see all the time.\nWill it rain tomorrow? Of course, a meteorologist would give you a forecast as a percentage. This would come from their subjective expertise as a meteorologist, supported by an enormous pile of modeled weather data to support their forecast.\nMany industries follow similar principles. This means:\n Risks are structured as scenarios. Reference class data is gathered as closely to the scenario as possible. Forecasts are made with all available information. Decisions are made. Corrections are made when compare forecasts to reality.  Here are some industry examples which use different language, but with the same principles.\nAerospace # The FAA has regulation about space launches. All of their risk management effort directs towards a measurement for a simple event: Expected Casualties (Ec).\n For all launches, regardless of vehicle type, this final rule requires a single expected number of casualties (Ec) be calculated by aggregating the risk posed to the collective members of the public\u0026hellip;\n  This final rule also revises the acceptable risk threshold for launch from an Ec of 30 × 10E-6 for each hazard to an Ec of 1 × 10E-4 for all three hazards combined.\n What does this mean in english?\nHow many casualties will result from a launch? Nuclear Safety # The NRC (US) certifies nuclear reactors partially based on the completion of Probabilistic Risk Analysis. Nuclear is concerned with events that may result in core damage (Level 1 PRA), how these may leak radioactivity (Level 2 PRA), and how individuals would be put at risk from these exposures (Level 3 PRA). Expert elicitation techniques in nuclear risk measurement are common.\n Will a tube in a steam generator rupture? Will there be health effects from resulting raditation leaks around a plant? How frequently will the core be damaged?  The nuclear industry relies on extensive data gathering to inform estimation methods, and expert opinion is relied on when adjustments need to be made for innovations without historic data.\nEnvironmental Safety # Environmental impact organizations use the probabilistic risk assessment.\nScenario: The change in the mortality rate due to Fine Particles (PM2.5) decrease if we pass X regulation.\nOutcome: Credible Interval: Reduction of .001 -.05% with 95% confidence.\nThe CSB organizes investigations that provide transparency into root causes informing probabilistic risk approaches supported in EPA policies.\nMeteorology # The United States spends billions on weather forecasting and its associated infrastructure.\nScenario: Will the east coast hurricane make landfall near our city before we can evacuate?\nOutcome: % Likelihood of Yes / No: Yes with a likelihood of 50%.\nNOAA and other global organizations build weather infrastructure that makes operational forecasting possible. Meteorologists still make adjustments to their models when publishing forecasts based on known failures of their models, outages in data collection, or local familiarity.\nIntelligence Analysis # All forms of intelligence gathering ultimately desire to inform decision making.\nScenario: Does the image taken by our satellite depict an adversary military aircraft?\nOutcome: % Likelihood of Yes / No: Yes with a likelihood of 70%.\nQuantitative estimates are a foundational part of the National Intelligence Estimate process, and are used globally by intelligence agencies. Publicly accessible platforms have taken similar approaches with decision markets accepting participation from large groups.\n"}).add({id:13,href:"/risk-measurement/docs/intro/measurement/",title:"Measurement",description:"The definition of measurement is probably more inclusive of methods than you would expect.\nPhilosophers, standards bodies, and scientists all hold that measurement is approximation. They all assume that uncertainty exists in measurement. They all endeavor to reduce it.\nWe often use an objective instrument to measure something, but this does not eliminate uncertainty. For instance, we may use a ruler to measure the size of a table. When an objective instrument does not exist\u0026hellip; we rely on increasingly unreliable methods.",content:"The definition of measurement is probably more inclusive of methods than you would expect.\nPhilosophers, standards bodies, and scientists all hold that measurement is approximation. They all assume that uncertainty exists in measurement. They all endeavor to reduce it.\nWe often use an objective instrument to measure something, but this does not eliminate uncertainty. For instance, we may use a ruler to measure the size of a table. When an objective instrument does not exist\u0026hellip; we rely on increasingly unreliable methods. For instance, a reference, historical statistics, estimations, and wild ass guesses.\nAn example: You want to move furniture. You have no tape measure.\nWhat do you do?\nYou\u0026rsquo;ll probably use your hands, feet, and eyes.\nThis is still measurement. Having a preference for a tape measure does not make other methods not measurement. Your individual preference for the best available measurement is not a demarcation of what gets to be a measurement.\nIn risk, impartial and direct measurements would be extremely valuable. Unfortunately, many of the risk events we care about haven\u0026rsquo;t happened yet.\nThey\u0026rsquo;re in the future!\nDirectly applicable instruments are not available to directly and impartially measure future events. Of course.\nMeasurements that are available to inform on a risk are indirect. They introduce subjectivity, and with that: partial views and cognitive bias.\nThe last mile of risk measurement is often Expert Elicitation from people who are well informed on the data and realities of a situation. This topic is referred to as adjustment, and is seen in multiple industries - though, handled with care, and under professional standards.\n"}).add({id:14,href:"/risk-measurement/docs/intro/for-noobs/",title:"New forecasters guide 🎉",description:"Do you want to forecast stuff? You\u0026rsquo;re in the right place!\nI don\u0026rsquo;t know how! # First, don\u0026rsquo;t worry! This is actually very simple.\nOverall, the process is forecast, talk, update forecast.\nFirst, get a forecast in. This is easy:\n All you\u0026rsquo;re being asked is to type some numbers. You won\u0026rsquo;t have to show or discuss your work. You\u0026rsquo;re not committing to anything long term. Forecasts are default anonymous. You will finalize it later, so just get something down.",content:"Do you want to forecast stuff? You\u0026rsquo;re in the right place!\nI don\u0026rsquo;t know how! # First, don\u0026rsquo;t worry! This is actually very simple.\nOverall, the process is forecast, talk, update forecast.\nFirst, get a forecast in. This is easy:\n All you\u0026rsquo;re being asked is to type some numbers. You won\u0026rsquo;t have to show or discuss your work. You\u0026rsquo;re not committing to anything long term. Forecasts are default anonymous. You will finalize it later, so just get something down.  Next, we talk about our forecasts. Talking is optional! You can lurk where the discussion is happening instead. What is important is that you think critically about the problem. Review other perspectives. Contributing to the discussion helps the whole process and is very much welcomed. You can share your actual forecast, or just your reasoning, or discuss points others have brought up.\nAfter you\u0026rsquo;ve had time to work through the problem with others, you have a chance to update your forecast.\nHere\u0026rsquo;s some discussion on the numbers you might be asked for.\nIf you\u0026rsquo;re being asked for an interval: That\u0026rsquo;s two numbers, a maximum and a minimum, assuming some percentage of confidence that the future value will land within it. That confidence level (like, say, 90%) is usually assigned and shared between everyone.\nFor example: Jelly beans in a jar: 100 (a minimum) to 200 (a maximum).\nIf you\u0026rsquo;re being asked about a scenario: That\u0026rsquo;s few probabilities that you believe some outcomes will happen over others.\nFor example: A 15% chance it will rain tomorrow.\nYou\u0026rsquo;ll probably be given a link to a forecasting tool (like e6e) or a spreadsheet to collaborate in.\nThere\u0026rsquo;s likely going to be some discussion about people\u0026rsquo;s forecasts that you can participate in as well. You might want to update your forecast after seeing discussion, and that\u0026rsquo;s very much encouraged.\nWhat happens next? # We wait for the actual outcome to occur. Then, we take a look at our individual and average forecasts. Hindsight analysis allows us to better understand what our beliefs were, why they may have been wrong, and how to improve them in the future.\nSome analysis will probably be written about the discussion, the measurements, and the actual outcome. These are usually shared more widely, while hiding individual participants (unless they want to step forward and discuss their forecasts).\nI want to do this more! # If you\u0026rsquo;re not added to the Substack, enter your information here.\n"}).add({id:15,href:"/risk-measurement/docs/intro/risk/",title:"Risk",description:"This section includes conceptual discussion to support your intuition around risk. A grasp of risk concepts will improve your ability to comprehend risk problems as well as articulate them to others.\nLanguage # This documentation mostly cares about quantitative risk, which has a strict definition (See Math below). However, the term risk has lots of useful interpretations. Miscommunication with others is common without clarifying what you are measuring.\nSven Ove Hansson describes risk with many definitions:",content:"This section includes conceptual discussion to support your intuition around risk. A grasp of risk concepts will improve your ability to comprehend risk problems as well as articulate them to others.\nLanguage # This documentation mostly cares about quantitative risk, which has a strict definition (See Math below). However, the term risk has lots of useful interpretations. Miscommunication with others is common without clarifying what you are measuring.\nSven Ove Hansson describes risk with many definitions:\n An unwanted event which may or may not occur. The cause of an unwanted event which may or may not occur. The probability of an unwanted event which may or may not occur. The statistical expectation value of an unwanted event which may or may not occur. The fact that a decision is made under conditions of known probabilities.  For example, playing russian roulette (1/8 odds) compared to walking through a literally haunted house (👻).\nThere are obviously more uses of the word risk.\n The lack of obvious mitigations (Driving without a seatbelt is a risk) An amount of property at stake (I have nothing to lose to the stock market!) General expressions of fear. (This feels pretty risky\u0026hellip;)  There are obviously consistent themes involved with all of these definitions. They all relate, somehow, to the likelihood of a future event, or the impact of a future event, or both.\nBe careful to spot different interpretations when discussing risk with others; You may think you\u0026rsquo;re talking about the same thing, when in fact, you\u0026rsquo;re talking past one another.\nMath # Traditional risk is represented as the following\nr = p * i The equation can be described as a scenario.\n r is Risk, or sometimes described as the Expected Value. p is a Probability involved with the scenario. i is an Impact.  Quantitative risk is composed from the product of how probable an undesireable event is, and the probable impacts that would result.\nWe very often focus on smaller components of this equation. Practical cybersecurity blue team work is full of examples where we make assumptions about a scenario or impact. \u0026ldquo;Assume Breach\u0026rdquo; is one of them, which you\u0026rsquo;ve likely heard of. Sometimes we assume a breach has already occurred, and work to mitigate downstream impact. Then, we may switch gears and patch a vulnerability that could cause that same breach to begin with.\nThis leaves us finding ourselves applying many hours understanding the probability of a scenario of an occurring\u0026hellip; while we make assumptions about impact. Or vice versa. This is why understanding the whole equation is necessary for understanding how everyone contributes to the reduction of risk.\nHowever, actual risk measurement work can be creative and flexible depending on our areas of interest.\nExpected Value # Let\u0026rsquo;s say our scenario states how much risk a museum\u0026rsquo;s assets ($500M) a museum has to a fire (1% odds) next year.\nrisk = $500M * 1%\nThe expected value (r) of losses would be $1M. If this scenario were simulated hundreds of thousands of times\u0026hellip; the average loss would also be about $1M\u0026hellip; the same value we would expect. So, the name \u0026ldquo;expected value\u0026rdquo; is quite helpfully chosen!\nThe expected value is useful in comparison with the expected value of other risks. You can compare the expected value of risks and prioritize them.\nAlso, mitigation costs can also be explored in comparison to the reduction of expected value. If a mitigation is more expensive than the reduction of a risk - it may not worth mitigating.\nExplained similarly: The expected value of coming up heads on a coin flip is 50%, which can either be calculated or simulated.\nProbability # The probability (p) of an event could come from a variety of places. Let\u0026rsquo;s use an example scenario of a fire taking place at a chosen museum next year.\nWhere would we gather a probability of this happening for use in analysis? The answer takes some creativity and some resourcefulness. There may be statistical data on fires at museums you could manually collect. Maybe the insurance company provides their estimate based on claims data. Perhaps risk managing employees at the museum perfomed a study and were elicited for a probability.\nYou\u0026rsquo;ll notice from these examples that probability is subjective! Even if we were to statistically calculate a likelihood from a useful dataset, it would still be up to us (the subject) to choose this indirect measurement as representative of a future probability. Accepting this data allows this data to suggest furture performance, when we already know that past results do not guarantee future performance.\nProbability used this way is considered a measure of evidence or belief, rather than a certification of future outcomes.\nProbabilities must follow the probability axioms. You\u0026rsquo;ll find more formal definitions elsewhere, but they\u0026rsquo;re explained here for context in risk modeling.\nFirst axiom: A probability has to be a non-negative real number.\n It\u0026rsquo;s not possible to be infinity percent sure of something, right? Similarly, you can\u0026rsquo;t be -5% sure, either.  Second axiom: All of the events in the sampled space must equal 1.\n Assume we\u0026rsquo;re predicting the winner of the Super Bowl. The combined odds of all of the candidates must equal one (100%). Since there\u0026rsquo;s two teams involved, the sum of each victory probability must equal 100%. If you\u0026rsquo;re estimating some percentage of a system being exploited, then the other half must sum to 1. 5% Yes + 95% No = 100%  So, all potential outcomes in an experiment must equal one. Or looked at the other way around: All possible outcomes must divvy up (and exhaust) the 100% odds. No leftovers!  Third axiom: For all events in a sampled space that are mutually exclusive, the result of their sum should equal their union.\nIn more simple english, imagine a deck of cards. There are no \u0026ldquo;Kings\u0026rdquo; that are \u0026ldquo;Queens\u0026rdquo;, because the face cards are mutually exclusive. No card is both a King and a Queen.\nSo, the odds of a card being a King (1 in 13) can be added to the odds of being a Queen (also, 1 in 13) safely (2 in 13 odds of being a King OR Queen)\nHowever, the odds of being a Heart and a King are not mutually exclusive, because they intersect with the King of Hearts which is simultanously both cards.\nSo, you cannot cleanly add the odds of being a King (1 in 13) with the odds of being a Heart (1 in 4), because there is a single card that could be both. 16 cards would meet the criteria of being a King or a Heart in reality. If you broke this rule and treated them as mutually exclusive, you might count the King of Hearts twice, or not at all. This is a failure of quantitative risk modeling.\n🚨  This axiom creates problems in real world risk measurement. Keep reading!  Enforcing this axiom (or ignoring it) creates weaknesses in probabilistic risk assessments. This property is something we work against in making broad risk models.\nIn cyber security: We often make models that treat events as mutually exclusive when they are not. As an example:\nWhile it is unlikely that an spearphishing APT group might work with an insider, these are not mutually exclusive events, and it\u0026rsquo;s certainly not outside the realm of possibility that it could occur.\nWe are easily suprised by what we see in the wild because it breaks our models, but our models may still be applicable as a proxy to organize efforts around them.\nHowever, we will always operate with models (quantitative or not) and cannot escape them. Quantitative modeling is clear on where they can be improved and corrected, which is where it beats qualitative methods. Qualitative methods might have benefits covering complex scenarios, but may not have the same measurement capability.\nImpact # Impact i represents losses. It could be as simple as a i = 1 to represent an all or nothing event, like death. Or, it could the value of a bank account, the passengers on a plane\u0026hellip; or from our previous example: The value of all art in a museum.\nHowever, impact is another probabilistic value with varying results. This will be covered in distributions. Here, let\u0026rsquo;s consider the behavior of some cybersecurity impacts:\n The absurd variance involved in breach costs An under-the-radar incident compared to sustained headlines in the press Prolonged civil litigation, versus a quick settlement The amount of customers that may, or may not, churn from an incident. The amount of time fixing a vulnerability.  When impact is a simple value-at-rest, like a bank account\u0026hellip; it can simply be represented by that value. When impact behaves probabilistically, we can resort to writing code as a monte carlo simulation.\nIt\u0026rsquo;s OK to assume that a scenario will take place and focus on studying its impact, just like it\u0026rsquo;s OK to assume that an incident is bad enough to warrant a study of its causes.\nRisk in practice # While r = p * i is foundational to risk measurement, it is not always the focus of our studies. We focus more practically on certain aspects of risk due to our interests or limitations. We are often surrounded by factors that shape our ability to influence a risk.\n Your time and resources available. The skillsets you can contribute to a problem. A short lived opportunity to mitigate a risk. You, or your team's role in the mitigation  As such, you might find yourself with many available strategies that have limitations associated with them. Or, you may be simply asked to use a specific strategy arbitrarily.\nHere is an example scenario:\n The vault will be robbed during overnight hours, in Q2, resulting in $1M loss or greater.  We face limitations as engineers that may hinder our ability to influence either the likelihoods or impacts that compose a risk.\nThis limits the ability of engineers to influence risk when using definitions of risk that attempt to strictly track the Expected Value of a risk, since the Expected Value requires that both factors are calculated.\n👉  The Expected Value of risk is equal to the product of its likelihood and impact. For example, a 50% likelihood of a $10 loss within a year has an expected value of $5 annually.  A fully rational approach to mitigating this risk would be to explore options that reduce the Expected Value of this risk.\nHowever, in reality, we may not be operating in a fully quantified environment. For instance, we might be the company that manufactures the walls of the vault. We don't get to know what will be stored inside of our vaults, but we do know how likely a certain type of drill would pierce the vault, given a certain amount of time.\nAlternatively, we may be in charge of customer use of the vault. Maybe this allows us to estimate the value of what is stored in the vault, but aren't tasked with understanding the likelihood of it being stolen.\nIndeed, many people involved with risk will need their own measurement tooling before Expected Value can be considered.\nThere are plenty of examples where engineers will not be calculating the full Expected Value of losses, but this does not limit their measurement opportunity to help inform this value. It's important to recognize that an engineer may be surfacing specific details for future management, and that these values are useful.\nThese sorts of measurement limitations are common in engineering. This documentation serves as guidance towards risk measurement in more practical situations so that they can be useful with broader risk management situations where expected values become useful while informing a broad picture of risk, and the decision making associated with it.\nThe world being as it is, the Expected Value of a risk is a desirable thing to measure, but it is unreasonable to think that all participants in risk mitigation will be fully concerned with it. Instead, we can trust that efforts to mitigate a risk may reduce the Expected Value of a loss, even if an engineer doesn't have the full picture measured.\nUncertainty and Risk # Uncertainty is a term that often comes up in risk measurement, and is often used interchangeably with probability. For example, the probability of some scenario occuring can be described as the uncertainty involved with that scenario.\nHowever, there are interpretations of risk that distinguish risk from uncertainty, as Knightian Uncertainty.\nThis interpretation of risk could certainly do us a favor and use different terminology. Nonetheless, the lessons drawn from this interpretation are important enough to cover for cybersecurity risk. It\u0026rsquo;s important to note that very well read minds disagree with whether Knightian risk actually exists, but it is still a good model for discussing the information surrounding a scenario.\nThrough this lens, you can view a risk as something that is deeply susceptible to measurement, and uncertainty as something that is not.\nKnightian Risk\nA game of Russian roulette can be considered a Knightian risk. Your odds are right there in the chamber; A six shooter has a 5 in 6 odds of survival, and 1 in 6 odds of ruin. Also, you can inspect the gun itself for flaws. A player can perform industrial radiography on the revolver and check for manufacturing flaws, and do the same with the bullet. Similar models can be tested for jams or other malfunctions. All said, Russian roulette can be a very well understood system to a potential player. However, knowing this system does not make the odds any favorable - it is still likely to be about 1 in 6, unless new information is found. Adding more resources and science to the study of the game does not make the game any more or less safe.\nKnightian Uncertainty\nSpending the night in a literally haunted house is an example of Knightian Uncertainty. You\u0026rsquo;d be unfamiliar with the houses inner-hauntings. You\u0026rsquo;d not have about previous victims who have survived a night in the house. Perhaps you only have some evidence of a terrifying event that took place that earned its reputation.\nWhile this is a humorous example, you can tell that scientific approaches are more difficult in this scenario, not cost efficient, or not invented yet. Until the scooby-doo crew comes in and reveals the inner workings of the haunted house, A rigorously studied probability is not available to make a decision about whether to stay in a haunted house or not. Someone could, perhaps, make a forecast about a life-or-death scenario within the house, but it will likely have a lot of noise between different individuals making the forecast as there is likely no statistical anchor between them.\nKnightian application to Cyber-Security # A useful interpretation of Knightian risk for cybersecurity is to discuss how much inspection or research is available for a certain scenario. There are obviously areas of cybersecurity that are deeply covered with measurement opportunities: audits, detection, logging, red teaming and all sorts of other analysis. These can be viewed as efforts that moves risks farther on a spectrum from unknown to known, and are more abundant in networks and areas that we control, or are well resourced.\nHowever, there are risks that do not have similar opportunities. These risks are more difficult to transition into more knowable problems. For instance:\n The security of your vendors may only be understood through a questionnaire. The machinations of your adversaries intentions may never be fully known. After an acquisition, the possibility of an existing breach can\u0026rsquo;t be fully eliminated.  Even well traveled, open source, and heavily tested code can have risks, so this is not to say that either side of Knightian interpretation is preferrable. Rather, we should be sensitive to the amount of study that has occurred behind the risks we are making decisions about, and value opportunities to measure risks that are hard to reach.\nDecision making should take into account the understanding of risks that lack appropriate study, versus well studied risks. See Decision Standards for thoughts on incorporating this level of rigor into a decision.\n"}).add({id:16,href:"/risk-measurement/docs/enterprise/organizing-risks/",title:"Organizing Risks",description:"An enterprise has a lot of varying risks. We want to organize them and work collaboratively to understand them.\nWhat we don\u0026rsquo;t want is a single person or team to assess and understand all the risks across an organization. Some federation of risk assessments need to occur, and we can do this by following the third axiom of probability when decompose a risk to share as a study.\nA single engineer working in isolation should be capable of modeling a risk and developing measurements for it.",content:"An enterprise has a lot of varying risks. We want to organize them and work collaboratively to understand them.\nWhat we don\u0026rsquo;t want is a single person or team to assess and understand all the risks across an organization. Some federation of risk assessments need to occur, and we can do this by following the third axiom of probability when decompose a risk to share as a study.\nA single engineer working in isolation should be capable of modeling a risk and developing measurements for it. However, organizations are likely to have lots of varying activities in flight that generate very different forms of risk.\nLet\u0026rsquo;s use a typical consumer web company for example. These companies share common risks. Here is an example model:\n Employee networks, applications, and devices. (Corp) Infrastructure providing compute, storage, and networking. (Infra) Applications that serve the business to consumers. (Product / App Sec) Business logic abuse upon those products. (Spam, ATO, InfoOps, Fraud) Regulatory, legal, and physical threats. (Phys. Sec, Law Enforcement, Legal)  Federated Risk Assessment and Analysis # Let\u0026rsquo;s say we begin a study of possible incidents that could occur which would require regulatory disclosure. We\u0026rsquo;ll call the scenario: SEV0 with Regulatory Disclosure.\nNow, ask yourself:\n What organization would be in command for our next SEV0 incident requiring disclosure?\n This approach to decomposition allows us to break down these risks into functional organizations. Even though each team works on pretty different subject matters, we have still unified everything as a single risk. This may be useful information to leadership.\ngraph TD; A[SEV0 with Regulatory Disclosure] A--\u0026gt;C[CorpSec]; A--\u0026gt;D[InfraSec]; A--\u0026gt;E[AppSec]; A--\u0026gt;F[ProdSec]; B[Other]; A --\u0026gt;B So, how would one of the above labels be applied to a SEV0?\nThis may be classified during an incident review, a postmortem or retrospective process, or simply a judgement by a CISO or other leader: \u0026ldquo;This was clearly an incident within Infra\u0026rdquo;.\nThe risk model allows us to federate this risk even further.\nFor example, we can request a study of CorpSec by others, in parallel.\nA separate individual or group can wholly own and investigate risks with a top level \u0026ldquo;SEV0 with Regulatory Disclosure\u0026rdquo; requirement. They can ask themselves, \u0026ldquo;What could take place within CorpSec that would result in a SEV0 and a disclosure?\u0026rdquo;\nNow, this team or organization can be introspective about their own risks while contributing to the larger organization\u0026rsquo;s goals.\nThis study might look something like the following:\ngraph TD; A[CorpSec: SEV0 with Regulatory Disclosure] Z[SEV0 with Regulatory Disclosure]--\u0026gt;A; A--\u0026gt;C[Endpoint Intrusion]; A--\u0026gt;D[Insider]; A--\u0026gt;D1[Corp App ATO]; A--\u0026gt;E[Removable Storage Theft]; A--\u0026gt;F[VPN Intrusion]; B[Other]; A --\u0026gt;B Because we\u0026rsquo;re decomposing the same organizational scenario, we can study the model as a whole organization.\nNow we can quantify each CorpSec scenario as a fraction of the overall risk from CorpSec, and CorpSec as a fraction of the overall risk of a SEV0 with regulatory disclosure. This can happen throughout the organization, if this is desireable.\nAd-Hoc Approaches and modeling failures # A federated assessment can suffer the same fate as any process that elevates a single metric too highly. Eventually the model begins to lose representation of what risks an organization is concerned about.\nThe previous example revolves around regulatory disclosure. What if some teams are focused on reputation damage, customer churn, or civil liability? These concerns may not be captured in the model. Be careful not to put a narrow risk on a pedestal.\nIn these cases it makes sense to federate studies with broad organizational concerns. However, avoid larger directional that may clobber into more focused efforts. Those efforts may not be well represented by a model, but may still be mitigating risk effectively. .\n"}).add({id:17,href:"/risk-measurement/docs/intro/scenarios/",title:"Scenarios",description:"We use scenarios to specifically define future events and impacts that we'd like to measure. This must happen before we start measurement, either through the collection of historic data or through expert elicitation.\nIn our discussion about risk we discussed the likelihood and impact of a future event, as r = p * i\nA point of failure in measurement is ambiguity about a risk problem being solved. A group of very smart people can spin in circles discussing a risk if they don't pin a specific outcome down first.",content:"We use scenarios to specifically define future events and impacts that we'd like to measure. This must happen before we start measurement, either through the collection of historic data or through expert elicitation.\nIn our discussion about risk we discussed the likelihood and impact of a future event, as r = p * i\nA point of failure in measurement is ambiguity about a risk problem being solved. A group of very smart people can spin in circles discussing a risk if they don't pin a specific outcome down first.\nUntil we define a scenario, r = p *i is not useful.\nTo do this, we explicitly define a risk with a scenario, which is an unambiguous statement about an undesireable future event.\nA scenario can have multiple outcomes to be estimated. For instance, an explosion may have costs, loss of life, or periods afterward spent offline.\nHere's a batch of examples:  Scenario : We disclose an incident to a regulator next quarter.   Scenario : Employees at a datacenter are unreachable during an incident next year.   Scenario : We are named in a lawsuit covered by a major newspaper next quarter.  Scenarios should be familiar because many industries advocate for the tabletop scenario as a way to encourage brainstorming and better understanding of risks.\nAlways include a specific timeframe. # We must include a specific timeframe with our scenarios. A risk can be viewed completely differently if it described as something happening tomorrow, or within the next ten years. Discussing scenarios without specific timeframes will cause communication and prioritization issues.\nView scenarios as a hierarchy. # A scenario has ties to the Fault Tree and Tree diagram, whereas higher level outcomes at the top of a tree aggregate the likelihoods of more specific events at the branches of a tree. Doing so keeps us compatible with quantitative risk methods, as it follows an axiom of probability.\nHere, we breakdown the methods that malicious code could have executed on an endpoint.\ngraph TD; A[Malware has executed on an endpoint] A--\u0026gt;C[A: Browser Exploit]; A--\u0026gt;D[B: User ran malicious binary]; A--\u0026gt;E[C: Hijacked App Dependency]; A--\u0026gt;F[D: Networked Service Exploit on Host] B[Other]; A --\u0026gt;B This is described as decomposing a risk. Decomposition offers flexibility in scenario building. With this flexibility, one could target broad failures, or more narrow ones, by being more or less precise with language.\nThe process of decomposing a risk with a group is sometimes the most useful part of quantitative risk exercises.\nHowever, a modeling assumption is that these decompositions are mutually exclusive and are not in violation of the third probabilistic axiom.\nHere\u0026rsquo;s an example of a model failure: The adversary could exploit a service on an endpoint over the network and (sigh) use this access to double click malware that they\u0026rsquo;ve emailed to same victim. Of course, this would be a bit roundabout and would bypass our model\u0026hellip; but there may be situations that do not.\nIn these cases, we would classify it as \u0026ldquo;Other\u0026rdquo;, which is a catch-all for failures of the imagination. Some models will have a larger presence of \u0026ldquo;Other\u0026rdquo; that others due to our familiarity with risks. We still classify incidents that occur in \u0026ldquo;Other\u0026rdquo; as an area for model improvement.\nThis weakness of probabilistic models is not necessarily a bad thing if it still drives mitigation efforts in the desired way.\nIn Enterprise this aspect of scenarios-informing-scenarios is used to inform larger organizational approaches to risk.\nA principle of scenario building (see: Limitation is to assume that unknown scenarios may occur. Erring towards upward investment in a hierarchy of scenarios helps defend against \u0026quot;unknown\u0026quot; branches. The initiating events that create complex problems can sometimes not be predicted, and assuming large forms of failure can help prevent disaster.\nWe can use this flexibility to model risks and measure them. We can decompose a risk with greater resource and available effort, but not so much that we lose sight of our risks and become vulnerable to uncertainty that was not accounted for.\nOutcomes and Judgments # One issue in forecasting is deciding on the criteria that \u0026quot;closes\u0026quot; a forecast. For instance:\n Scenario : We disclose an incident to a regulator next year.  This scenario is simple to judge, as you would likely respect the judgment your legal counsel as to whether this occurred. No trouble here. Though, is important to identify how a scenario will be judged.\nThe \u0026quot;judge\u0026quot; becomes part of the forecast, and may influence the certainty of the forecasters if the judge is biased or unreliable.\nThe judges that are selected to evaluate outcomes should be considered for their impartiality to the outcome, and we want scenarios to be very specific.\nDesignate a team or individual to pass judgment on an outcome when no clear judgment exists.\nIf there is concern that a Black Swan may invalidate the forecast, it is best to make sure the forecastable outcomes include \u0026quot;other\u0026quot; circumstances. For instance:\n We disclose an incident next year. We don\u0026rsquo;t disclose an incident next year. Other  This would allow you to continue judgement under difficult to imagine circumstances. Additionally, decisions can reverse. Write defensible scenarios that mitigate flip-flopping of an outcome.\nPerhaps the regulator decides that the incident does not qualify as an incident, and there is a desire to retroactively invalidate the scenario.\nThe reliability of judgment can also be bolstered to decision makers if included in whistleblowing policy or professional codes of conduct. (See: whistleblowing)\n"}).add({id:18,href:"/risk-measurement/docs/enterprise/decision-standards/",title:"Decision Standards",description:"Carl Sagan once said \u0026quot;Extraordinary claims require extraordinary evidence\u0026quot;. For us, extraordinary decisions follow the same principle.\nLeadership can require that a risk measurement meet certain requirements that defend it against negligence, cognitive error, corruption, imaginative failures, or gaps of evidence.\nHere are some example thoughts on standardizing the level of expense expected for a measurement.\n👉  For a real-world example, see NASA-STD-7009  Level Zero: An individual, \u0026ldquo;water cooler\u0026rdquo; estimate.",content:"Carl Sagan once said \u0026quot;Extraordinary claims require extraordinary evidence\u0026quot;. For us, extraordinary decisions follow the same principle.\nLeadership can require that a risk measurement meet certain requirements that defend it against negligence, cognitive error, corruption, imaginative failures, or gaps of evidence.\nHere are some example thoughts on standardizing the level of expense expected for a measurement.\n👉  For a real-world example, see NASA-STD-7009  Level Zero: An individual, \u0026ldquo;water cooler\u0026rdquo; estimate.\nLevel One: A estimate from a single individual who has received risk training, has particular subject matter expertise, reasonable access to reference classes, or has specific historical data.\nLevel Two: In addition to previous levels, this estimation is from an individual who is calibration trained (See: calibration) and maintained a track record. (See keeping score)\nLevel Three: In addition to previous levels, a panel of three or more calibrated individuals were involved with this estimation.\nLevel Four: In addition to previous levels, an external expert was involved with the estimation.\nYou can, of course, stack and order these however you\u0026rsquo;d like. Each example simply increases the rigor involved, approaching greater scientific integrity. Some of these qualities of rigor are listed below:\nRigor # If a leveling approach is not desired, the following criteria can be requested as part of a study as needed:\n Risks are decomposed and their components are individually (or independently) studied. Forecasts are elicted by individuals who have been calibration trained. Forecasters have a determined track record with Brier scores meeting a specific threshold. (See: Keeping Score) Historical data was sought out and modeled with frequentist approaches to inform forecasts. Measurement of \u0026quot;unknown\u0026quot; factors were a part of forecasts. A panel of individuals were relied on instead of a single individual. Critique, feedback, and whistleblowing have formal process. Outside expertise is leveraged. Monte Carlos simulations are developed and studied.  Rigor is not required for all measurements. Rather, important decisions should clearly identify what amount of rigor is expected in any measurement effort. Measurements can happen verbally at a water cooler, or with concerted effort in order to inform an important decision.\nFurther Reading # See Expert Groups, Cognitive Error, Industry\n"}).add({id:19,href:"/risk-measurement/docs/enterprise/kpi/",title:"Performance",description:"Managers quickly look towards performance management when metrics become available.\nHowever, an available method to quantify risks may not translate well for measuring individual or organizational performance. Trouble awaits for those who assign incentives (bonuses, promotions, etc) for directionally favorable changes in risk measurements, especially ones that rely on expert elicitation.\nKey Results (OKR) and KPIs are still valuable leadership tools that assume values (in this case, a risk measurement) may correlate with some notion of success.",content:"Managers quickly look towards performance management when metrics become available.\nHowever, an available method to quantify risks may not translate well for measuring individual or organizational performance. Trouble awaits for those who assign incentives (bonuses, promotions, etc) for directionally favorable changes in risk measurements, especially ones that rely on expert elicitation.\nKey Results (OKR) and KPIs are still valuable leadership tools that assume values (in this case, a risk measurement) may correlate with some notion of success. While these approaches have leadership value, there are rough edges.Metrics based management is especially dangerous with risk metrics.\nHere is a strawman example:\n Our goal is to lower the probability and impact of a disclosable SEV0 incident.\n While a management direction towards reducing this risk may be healthy, it may not be healthy when used as a tool for directly measuring performance.\nKPIs are subject to organizational toxicity whether they are objectively measurable or subjectively approximated through expert elicitation.\nAn organization should avoid \u0026quot;risk\u0026quot; being valued as the individual performance measure.\nInstead, a performance management focus on the completion of activities that measure, discover, and mitigate risk is more attractive for performance assessment. These contributions should be viewed independently of the risk measure. In other words, let the scoreboard handle itself.\n"}).add({id:20,href:"/risk-measurement/docs/other/reading-material/",title:"Reading Material",description:"This document contains reading material bolstering the estimation, expert elicitation, quantitative certainty, scenario, and forecasting subject matters that guide Risk Measurement.\nRisk Language # This section includes reading that helps navigate the problems with \u0026quot;risk\u0026quot; language, miscommunications and so forth.\nDefining Risk # The language of risk is used in a variety of ways and shows up with different intentions in practice.\n Hansson, Sven Ove, “Risk”, The Stanford Encyclopedia of Philosophy (Fall 2018 Edition), Edward N.",content:"This document contains reading material bolstering the estimation, expert elicitation, quantitative certainty, scenario, and forecasting subject matters that guide Risk Measurement.\nRisk Language # This section includes reading that helps navigate the problems with \u0026quot;risk\u0026quot; language, miscommunications and so forth.\nDefining Risk # The language of risk is used in a variety of ways and shows up with different intentions in practice.\n Hansson, Sven Ove, “Risk”, The Stanford Encyclopedia of Philosophy (Fall 2018 Edition), Edward N. Zalta Boholm, M. , Möller, N. and Hansson, S. O. (2016), The Concepts of Risk, Safety, and Security: Applications in Everyday Language. Risk Analysis, 36: 320-338. doi:10.1111/risa.12464  Specific Scenarios # This section contains supporting reading for specific scenario building. The \u0026quot;Scenario\u0026quot; is frequently used language in modern approaches to risk analysis.\n The Kaplan and Garrick Definition of Risk and its Application to Managerial Decision Problems  This form of the scenario is produced immediately from fault tree analyses and similar methods associated with engineering safety studies (i.e., nuclear reactor safety studies). Each scenario represents a unique concatenation of events.   Clarity Test Decision Analysis: Practice and Promise. Ronald A. Howard. Management Science, Vol. 34, No. 6. (Jun., 1988), pp. 679-695.  Hierarchy of Scenarios # The hierarchal relationship between specific future events. This is related by the third axiom of probility, that decomposition of a scenario should have \u0026ldquo;disjoint sets\u0026rdquo; or should be mutually exclusive from one another.\n Fault tree analysis Schneier on Security Fault Tree Handbook with AeroSpace Applications Amoroso, E. G. (1999). Fundamentals of computer security technology Swiderski, F., \u0026amp; Snyder, W. (2009). Threat Modeling Shostack, A. (2014). Threat modeling: Designing for security  Measurement / Approximation # This section includes all references to, and arguments that measurements are estimates. Generally speaking, everything we do is some form of approximation, even when employing the use of measurement instruments.\n Measurement Uncertainty Antonio Possolo [Tal, Eran, “Measurement in Science”, The Stanford Encyclopedia of Philosophy](\u0026lt;https://plato.stanford.edu/archives/fall2017/entries/measurement-science/) Hubbard, Douglas W., and Richard Seiersen. How to measure anything in cybersecurity risk A Turning Point for Humanity: Redefining the World’s Measurement System Robin Materese Keeping the Standard Kilogram From Gaining Weight Is a Constant Struggle Nadia Drake Why scientists are redefining the kilogram Kibble balance Evaluation of measurement data – Guide to the expression of uncertainty in measurement JCGM 100:2008 (GUM 1995 with minor corrections)  Expert Estimation # This section generally appeals to how experts can be queried for quantitative data.\nCombining Expert Estimations # This describes the practice of gathering up forecast material and, typically, averaging it together. Parimutual Betting, Simple Averages, Weighted Scores.\n Parimutuel betting Probabilistic Forecasts and Reproducing Scoring Systems  Calibration of Experts # Also see [How to measure anything](#How to measure anything).\n Calibration Of Probabilities: The State Of the Art To 1980 Calibration Of Probabilities: The State Of the Art  Humorous Examples #  Miyamoto’s Secret Hobby Shigeru Miyamoto Guesses The Size of Random Objects (Jimmy Fallon)  RAND # RAND has been developing methods for expert estimation for decades, described as DELPHI and Futures Methodology.\n Publications on Futures Methodologies: Delphi Probabilistic Forecasts and Reproducing Scoring Systems On the Epistemology of the Inexact Sciences An Experimental Application of the Delphi Method to the Use of Experts The Systematic Use of Expert Judgment in Operations Research Convergence of Expert Consensus Through Feedback Improving the Reliability of Estimates Obtained from a Consensus of Experts The Use of the Delphi Technique in Problems of Educational Innovations Analysis of the Future Delphi Systematic Use of Expert Opinions Delphi Process Experiments in Group Prediction Predicting the Future Delphi and Values The Delphi Method The DELPHI Method, II The Delphi Method, III The Delphi Method, IV Experimental Assessment of Delphi Procedures with Group Value Judgments Comparison of Group Judgment Techniques with Short-Range Predictions and Almanac Questions Delphi Assessment  Expert Groups # Also see Tetlock.\n Stan Kaplan, ‘Expert information’ versus ‘expert opinions’. Another approach to the problem of eliciting/ combining/using expert knowledge in PRA, Reliability Engineering \u0026amp; System Safety, Volume 35, Issue 1, 1992, Pages 61-72, R.L. Keeney ; D. von Winterfeldt. Eliciting probabilities from experts in complex technical problems, IEEE Transactions on Engineering Management ( Volume: 38 , Issue: 3 , Aug 1991 )  IARPA # IARPA invests in quite a bit of predictive research and publishes results often. They are also involved in forecasting tournaments.\n Teams Better Than Individuals at Intelligence Analysis, Research Finds. American Psychological Association.*  Cooke's \u0026quot;Classical Method\u0026quot; # Often found in environmental risk (Volcanic, Earthquake) and others.\n Roger Cooke, Max Mendel, Wim Thijs, Calibration and information in expert resolution; a classical approach, Automatica, Volume 24, Issue 1, 1988, Pages 87-93, ISSN 0005-1098 Abigail R Colson, Roger M Cooke; Expert Elicitation: Using the Classical Model to Validate Experts’ Judgments, Review of Environmental Economics and Policy, Volume 12, Issue 1, 1 February 2018, Pages 113–132 A route to more tractable expert advice, Willy Aspinall. Usgs Expert Elicitation Report Workshop on the ground motion models applied in the National Seismic Hazard Maps  Constructive critique of Cooke's method can be found here:\n Bolger, F. and Rowe, G. (2015), The Aggregation of Expert Judgment: Do Good Things Come to Those Who Weight?. Risk Analysis, 35: 5-11. doi:10.1111/risa.12272  Forecasting # Philip Tetlock # Tetlock's research revolves around how experts who are untrained in prediction are worse than random. He has since isolated those who are stronger forecasters (Superforecasters) and is identifying their qualities, especially around how someone a better forecaster, and how to further improve them with teams.\nMeteorology # Maybe the oldest area of forecasting. Understanding the industrial development of meteorology is a great rubric for how a predictive industry is built over time. First, the theory. Then the infrastructure. Then the operational practice of prediction, decision making, and learning.\n Weather Analysis and Forecasting Timeline of meteorology Bauer, Peter \u0026amp; Thorpe, Alan \u0026amp; Brunet, Gilbert. (2015). The quiet revolution of numerical weather prediction  Cognitive Error # Kahneman / Tversky # Daniel Kahneman and Amos Tversky offer observations into how fallible the human mind is in the most common of circumstances. The classification of System 1 and System 2 thinking is highly relevant to this area of critical thinking around risk.\n Kahneman, D. (2015). Thinking, fast and slow.  Meehl / Dawes # Paul E. Meehl and Robyn Dawes work in prediction inspired a full fledged assault on the credibility of expert prediction. Comprehensive findings that mechanical statistical models beat experts at prediction.\n Meehl, P. E. (1966). Clinical versus Statistical Prediction  N. Taleb # Taleb explores the limitations of our ability to understand randomness and the nature of randomness. Preparation for inevitable surprise, and the emergence of Black Swans, is Taleb's core message.\n Taleb, N. N., Taleb, N. N., Taleb, N. N., Taleb, N. N., \u0026amp; Taleb, N. N. (2016). Incerto.  Intelligence Analysis # Sherman Kent # Sherman Kent is considered a pioneer of intelligence analysis, and brought probabilistic rigor into the National Intelligence Estimate.\n Sherman Kent  His writing:\n Words Of Estimative Probability The Law and Custom of the National Intelligence Estimate The Making of an NIE The Theory of Intelligence  Canadian Intelligence # There is research around Canada's application of modern intelligence processing and its effectiveness. The basis of this is all probabilistic.\n Canada Is Actually Pretty Good At Intelligence Forecasting, Ben Makuch Accuracy Of Forecasts in Strategic Intelligence, David Mandel-Alan Barnes  Industry Examples # Industry examples where probabilistic risk assessment is at play:\n NASA Risk Management Handbook EPA: Risk Assessment Forum White Paper: Probabilistic Risk Assessment Methods and Case Studies Probabilistic Risk Assessment Procedures Guide for Offshore Applications Nuclear Probabilistic Risk Assessment  This paper has a specifically useful overview of many different industry approaches to safety.\n White Paper on Approaches to Safety Engineering  "}).add({id:21,href:"/risk-measurement/docs/estimation/strategy/",title:"Strategy",description:"Estimation is a skill that benefits from approximation strategies. These mental models will help support common problems in risk modeling, elicitation, and forecasting. They\u0026rsquo;re also useful as elicitation techniques when trying to draw out numeric estimates from an expert.\nDivide and Choose # Divide and choose is a mental heuristic to determine if odds are fair or not. It is similar to the children's \u0026quot;fairness\u0026quot; concept:\n One child slices a piece of cake Another child chooses the slice they'd like.",content:"Estimation is a skill that benefits from approximation strategies. These mental models will help support common problems in risk modeling, elicitation, and forecasting. They\u0026rsquo;re also useful as elicitation techniques when trying to draw out numeric estimates from an expert.\nDivide and Choose # Divide and choose is a mental heuristic to determine if odds are fair or not. It is similar to the children's \u0026quot;fairness\u0026quot; concept:\n One child slices a piece of cake Another child chooses the slice they'd like.  Of course, this method prevents the first child from slicing unevenly and taking the larger piece and forces them to behave impartially to the outcome. Being partial would result in them receiving a smaller slice.\nThe ability to impartially estimate odds is crucial to forecasting and decision making. Opportunities for advantageous decision making arise when rewards are not congruent with the odds, but the ability to see these impartially can be difficult.\nIn summary - what odds would you apply to a situation, where you would be comfortable taking either side fo the bet?\nPrinciple of Indifference # The principle of indifference is a rule of thumb that divides probability across all of its options. For instance, two outcomes at 50/50% or four outcomes at 25/25/25/25% respectively.\nThis principle is similar to the uniform distribution.\nWhen faced with a scenario with four outcomes, this principle suggests starting with 25% forecasted probabilities for each of outcome and continuing a study from there. Assuming there is no available information to suggest one outcome over another, this would be the most efficient default strategy to avoid error.\nDuring elicitation, an expert may immediately disagree with these odds. They may find themselves confronting information about their beliefs for each scenario and debating an indifferent strategy. Afterward, it's likely that the forecaster has opinions they can more easily express numerically: this should be higher or that must be lower.\nThe Absurdity Test # An absurdity test would assigns extreme and irrationally formed likelihoods or values to a forecast, testing the opinions of a forecaster. For instance, \u0026quot;A small child can eat between zero and one million pies in a sitting.\u0026quot;\nWhen faced with such a test, a forecaster may be encouraged to start making a forecast less absurd. For instance \u0026quot;Well, a child can at least eat half of a pie, and maybe up to five pies, in extraordinary circumstances.\u0026quot;\nThis form of test has been used as an interview prompt in psychological research since the 1900s.\nInside and Outside Views # An outside view looks at a specific situation as a frequently occurring pattern with a base rate. An inside view inspects why a specific situation is an exception from the norm. Balancing these views can offer a rational approach to understanding the probability of an event occurring.\nTetlock\u0026rsquo;s books heavily visit the concept of inside and outside views. Let\u0026rsquo;s assume we are measuring how often we\u0026rsquo;ll suffer a major data breach.\nOutside View: How often do companies in my sector suffer major data breaches? Inside View: What makes my company different than the rest?\nAn outside view may help regulate any instinct to panic over your risk findings. An inside view gives you the opportunity to compare them to how often they may be compromised in the wild.\nAvoid the Availability Heuristic. # The availability heuristic influences and biases a forecast due to a shortcut most thought processes take: if you remember it, it must be important. Keep in mind that the events in your memory do not represent all available data, nor does it represent the most important data necessary for a good forecast. It just happens to be your memory at the time.\nFor instance, a violent plane crash may bias people against flying, when flights are historically known to be very safe.\nIdentify your stubborness. # The bias of belief perseverance describes a belief you may have in spite of information that directly contradicts it. The causes of this bias are unclear. Your ability to suspend a belief as you explore opposing arguments is an important skill. It\u0026rsquo;s also important to avoid bold and confident claims when you lack supporting evidence, or have not thoroughly explored an opposing side.\nIn once extreme case, psychologists joined a cult, and observed that its members clung to their beliefs even after the world did not end on a given date.\n Festinger, Leon; et al. (1956). When Prophecy Fails. Minneapolis: University of Minnesota Press.\n CHAMPS KNOW # From research described here, here, and here.\n   Letter Probabilistic Reasoning Principal     C Comparison classes   H Hunt for the right information   A Adjust and update forecasts when appropriate   M Mathematical and statistical models   P Post-mortem analysis   S Select the right level of effort to devote to each question    Political Reasoning Principal   K Know the power players and their preferences   N Norms and protocols of institutions   O Other perspectives should inform forecasts   W Wildcards, accidents, and black swans    Comparison Classes (link) # Think through comparison classes to narrow in your forecast based on similar, comparable data.\n Assume that your scenario is not entirely unique, and find comparables. Identify what aspects of the scenario do make it unique. Explore the comparable scenarios and any data available, and determine how different they actually are by the estimating the value of those differences.  For instance: Estimating attendance at a performer\u0026rsquo;s first headliner gig. Can you explore data from concerts at the same venue, type of music, or audience demographic? Maybe their previous, non-headline shows will give some basis for reasoning as well.\nHunt for the right information # Immediately hunt for information that contradicts your gut feelings. Understand all nuance in the question statement. Be confident that you understand any nuance in the judgement criteria that may impact the outcome.\nAdjust and update forecasts when appropriate # If your scenario allows you to update a forecast over time, it will help. Not only is the progression of time a new source of information, but being able to \u0026ldquo;sleep on it\u0026rdquo; will allow you to apply critical thinking to a scenario with fresh perspective, or apply any other information you have gained.\nMathematical and Statistical Models # When possible, bust out your spreadsheet and calculator and explore the historical data involved with your scenario. Understand the limitations of mechanical modeling, as well as your own subjective forecasting. You, as a forecaster, are subject to bias. However, a mechanical model might not see the data it needs to capture probability correctly.\nPost-Mortem Analysis # Value the \u0026ldquo;busted\u0026rdquo; forecasts! They are precious. What influenced a forecast that was ultimately incorrect? What reasoning was incorrect, or undervalued? What data would have valuable to have, in hindsight?\nSelect the right level of effort to devote to each question # Very intentionally ask questions about the scenario. Decide which ones deserve your effort to research and answer them.\nKnow the power players and their institutions # (Related to political forecasting)\nNorms and protocols of institutions # (Related to political forecasting)\nOther perspectives should inform forecasts # How would someone else look at this scenario? Would someone invested in this scenario view it differently? What about people invested in different outcomes, what questions or data would they rely on?\nWildcards, accidents, and black swans # Always, always leave room for the complete shocker. Consider that we always view black swans as explainable events after they\u0026rsquo;ve punched us in the jaw. If you express certainty (0%/100%), you are likely opening yourself up for tragedy. The world is full of surprises, disappointment, accidents, and the unknown unknown.\nOnline Training # Participation in something continuous like the Good Judgement Open will help keep forecasting skills sharp. Otherwise, varying levels of rigorous training below are available.\n Good Judgement (recommended) Hubbard Research (enterprise) https://www.evidentmethod.com/training/ (free / enterprise) http://confidence.success-equation.com/  "}).add({id:22,href:"/risk-measurement/docs/estimation/distributions/",title:"Distributions",description:"Nearly all of the estimation methods in this documentation so far are actually components that could be obtained from a probability distribution if one were available. You\u0026rsquo;re likely familiar with the well known normal distribution, shown below:\n  Other topics discussed so far are simple components of a probability distribution.\n A percentile is the sum of all probabilities up to a value on a distribution. An interval communicates two percentiles from a distribution.",content:"Nearly all of the estimation methods in this documentation so far are actually components that could be obtained from a probability distribution if one were available. You\u0026rsquo;re likely familiar with the well known normal distribution, shown below:\n  Other topics discussed so far are simple components of a probability distribution.\n A percentile is the sum of all probabilities up to a value on a distribution. An interval communicates two percentiles from a distribution. For instance, the 5% and 95% percentiles, and simply describing the probability of a value occuring within them. (For instance, a 90% interval summarizes values in between 95% and 5%)  There are plentiful options for studying a probability distribution elsewhere, and this documentation will focus on intuition for their use in risk measurement.\n🐍  See the elicited library for a helpful layer to elicit parameters for these distributions.  Several distributions are useful when little is known about the behavior of an unknown risk. For instance:\nA uniform distribution only requires a minimum and maximum boundary, and evenly spreads outcomes across all values within.\nA triangular distribution requires a minimum, a maximum, and a mode. Probability is most dense near the mode value and will not exceed the minimum or maximum.\nA PERT distribution similarly uses a minimum, maximum, and mode. PERT is different from a triangular distribution that it includes a tail of values beyond the minimum and maximum values of a triangular distribution, and is continuous, and can be modified depending on goals.\nInformation Security Risk # The following section references distributions used in information security studies.\nIncident Frequency #  Poisson or negative binomial: Martin Eling and Nicola Loperfido. 2017. Data breaches: Goodness of fit, pricing, and risk measurement. Insur. Math. Econ. 75 (2017), 126–136.  Losses #  Gamma: The County Fair Cyber Loss Distribution: Drawing Inferences from Insurance Prices Power Law: Thomas Maillart and Didier Sornette. 2010. Heavy-tailed distribution of cyber-risks. Eur. Phys. J. B 75, 3 (2010), 357–364. Lognormal : Benjamin Edwards, Steven Hofmeyr, and Stephanie Forrest. 2016. Hype and heavy tails: A closer look at data breaches. J. Cybersecur. 2, 1 (2016), 3–14 Pareto: Spencer Wheatley, Thomas Maillart, and Didier Sornette. 2016. The extreme risk of personal data breaches and the erosion of privacy. Eur. Phys. J. B 89, 1 (2016), 7. Log-skew-normal: Martin Eling and Nicola Loperfido. 2017. Data breaches: Goodness of fit, pricing, and risk measurement. Insur. Math. Econ. 75 (2017), 126–136. Lognormal and Generalized Pareto: Antoine Bouveret. 2018. Cyber Risk for the Financial Sector: A Framework for Quantitative Assessment. International Monetary Fund.  Bugs / Vulns / Bounty #  Power Law: Given Enough Eyeballs, All Bugs Are Shallow? Revisiting Eric Raymond with Bug Bounty Programs Poisson: Enter the Hydra: Towards Principled Bug Bounties and Exploit-Resistant Smart Contracts∗   Message @magoo for anything that should be added.\n "}).add({id:23,href:"/risk-measurement/docs/intro/decisions/",title:"Decisions",description:"Decisions are subjective. This documentation is about making quantitative risk measurements. These measurements will not make decision making an objective process. We are still bounded by rationality. Risk based measurements will only be a subset of all information available for a decision.\nThis documentation can only offer methods to improve the quality of risk based measurements for those decisions, and offer commentary on decision processes themselves.\nData driven decisions # Often discussed is the data driven decision.",content:"Decisions are subjective. This documentation is about making quantitative risk measurements. These measurements will not make decision making an objective process. We are still bounded by rationality. Risk based measurements will only be a subset of all information available for a decision.\nThis documentation can only offer methods to improve the quality of risk based measurements for those decisions, and offer commentary on decision processes themselves.\nData driven decisions # Often discussed is the data driven decision. We may be subject to unconscious bias if we seek and collect data before structuring the criteria we are making a decision with.\nRather, we can pursue decisions as follows:\n Identify the problem. Set goalposts. Collect measurements.  For instance, we\u0026rsquo;ll examime a typical IT endpoint security problem at some large engineering organizations.\nThe Problem:\n Engineers are removing mobile device management software from their laptops, and in some cases, changing settings that violate policy or accumulating vulnerabilities we cannot track and mitigate. They\u0026rsquo;re also out of scope for our detection efforts - we can\u0026rsquo;t see them.\n Yikes! But, do we take action immediately? That\u0026rsquo;s the decision we have to make. There\u0026rsquo;s a lot of security work to do, what says that this is the first item to attend to?\nLet\u0026rsquo;s set some criteria that dictates at what point we begin taking action - now, or later.\nThe Goalposts:\nLet\u0026rsquo;s pretend we have thought on the issue, and determined some break points where the problem is too severe to ignore any longer.\n When more than 2% of engineering laptops become unmanaged, we\u0026rsquo;ll treat it like an incident and start enforcing a policy that denies them access to production and prioritize it over other work items.\n Now we have taken note of what types of action we would make before we\u0026rsquo;re biased with the data we\u0026rsquo;ve collected.\nThe Measurement:\n OK. Since you\u0026rsquo;ve asked, we\u0026rsquo;ve identified at least 5% of engineering laptops are connecting to the bastion environment without their hostnames appearing in MDM.\n This exceeds the goalposts we set. We can make the decision to begin enforcing the policy without being accused of having made a decision without regard for the data.\nThe McNamara Fallacy # This documentation is focused on quantitative risk, but warns of narrow mindedness in its usage. An overly focused attititude towards quantitative reasoning is sometimes referred to as the McNamara Fallacy. Individuals who reject information that hasn\u0026rsquo;t yet been quantified are vulnerable to this sort of failure. Qualitative analysis is still valuable.\nMetrics are gathered because they\u0026rsquo;ve met some level of accessibility. A unavailable metric or quantified measurement is not necessarily less important - there may be resource constraints in acquiring them.\nLeadership roles should be familiar with decision making under risk: Even with risk measurements, a total picture should be considered.\n"}).add({id:24,href:"/risk-measurement/docs/enterprise/",title:"Help",description:"Enterprise risk index",content:""}).add({id:25,href:"/risk-measurement/docs/other/",title:"Help",description:"Help Doks.",content:""}).add({id:26,href:"/risk-measurement/docs/examples/",title:"Example Index",description:"Risk measurement examples",content:"Risk measurement examples\n"}).add({id:27,href:"/risk-measurement/docs/intro/",title:"Introduction",description:"",content:""}).add({id:28,href:"/risk-measurement/docs/",title:"Docs",description:"Docs Doks.",content:"Select a topic below to get started.\n"}),search.addEventListener('input',b,!0);function b(){var b,e;const d=5;b=this.value,e=a.search(b,{limit:d,enrich:!0});const c=new Map;for(const a of e.flatMap(a=>a.result)){if(c.has(a.doc.href))continue;c.set(a.doc.href,a.doc)}if(suggestions.innerHTML="",suggestions.classList.remove('d-none'),c.size===0&&b){const a=document.createElement('div');a.innerHTML=`No results for "<strong>${b}</strong>"`,a.classList.add("suggestion__no-results"),suggestions.appendChild(a);return}for(const[h,g]of c){const b=document.createElement('div');suggestions.appendChild(b);const a=document.createElement('a');a.href=h,b.appendChild(a);const e=document.createElement('span');e.textContent=g.title,e.classList.add("suggestion__title"),a.appendChild(e);const f=document.createElement('span');if(f.textContent=g.description,f.classList.add("suggestion__description"),a.appendChild(f),suggestions.appendChild(b),suggestions.childElementCount==d)break}}})()